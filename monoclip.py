import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import sys
import clip

# --- ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ…Ù¾ÙˆØ±Øª SAM 3 ---
try:
    from sam3.model_builder import build_sam3_image_model
    # print("âœ… SAM 3 library imported successfully.")
except ImportError as e:
    print(f"âŒ Error importing SAM 3: {e}")
    sys.exit(1)

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¯Ø³ØªÚ¯Ø§Ù‡ ---
# Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù¾Ø± Ø´Ø¯Ù† Ø­Ø§ÙØ¸Ù‡ GPU Ù‡Ù†Ú¯Ø§Ù… Ù„ÙˆØ¯ Ø§ÙˆÙ„ÛŒÙ‡ØŒ Ù…Ø¯Ù„ Ø±Ø§ Ø±ÙˆÛŒ CPU Ù„ÙˆØ¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
LOAD_DEVICE = 'cpu'
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø³ÛŒØ± Ùˆ Ù…Ø¯Ù„ ---
SAM3_CHECKPOINT = "/home/ram112/projects/def-jieliang/ram112/checkpoints/sam3_large.pth"

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª DepthCLIP
depth_templates = ['This {} is {}']
obj_classes = ['object']
depth_classes = ['giant', 'extremely close', 'close', 'not in distance', 'a little remote', 'far', 'unseen']
bin_list = [1.00, 1.50, 2.00, 2.25, 2.50, 2.75, 3.00]
temperature = 0.1

class SAM3Encoder(nn.Module):
    def __init__(self, checkpoint_path):
        super().__init__()
        print(f"Loading SAM 3 Image Model from: {checkpoint_path}")
        
        # 1. Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ø±ÙˆÛŒ CPU Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡
        self.model = build_sam3_image_model(
            checkpoint_path=checkpoint_path,
            device=LOAD_DEVICE,  
            eval_mode=True,
            enable_segmentation=False,
            enable_inst_interactivity=False
        )
        
        # 2. Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ GPU Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
        if DEVICE == 'cuda':
            self.model.to(DEVICE)

        # 3. Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ú©ÙˆØ¯Ø± ØªØµÙˆÛŒØ± (Smart Backbone Detection)
        if hasattr(self.model.backbone, 'visual'):
            self.image_encoder = self.model.backbone.visual
        elif hasattr(self.model.backbone, 'trunk'):
            self.image_encoder = self.model.backbone.trunk
        else:
            self.image_encoder = self.model.backbone
        
        # ÙØ±ÛŒØ² Ú©Ø±Ø¯Ù† Ú©Ø§Ù…Ù„ SAM
        for param in self.model.parameters():
            param.requires_grad = False

    def forward(self, x):
        """
        ÙˆØ±ÙˆØ¯ÛŒ: ØªÙ†Ø³ÙˆØ± ØªØµÙˆÛŒØ± (B, 3, H, W)
        """
        batch_size = x.shape[0]
        dummy_captions = [''] * batch_size
        
        # ØªØºÛŒÛŒØ± Ø³Ø§ÛŒØ² Ø¨Ø±Ø§ÛŒ RoPE (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ SAM Ø±ÙˆÛŒ Û±Û°Û²Û´ ÛŒØ§ Û±Û°Û°Û¸ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯)
        if x.shape[-2:] != (1008, 1008):
            x_in = F.interpolate(x, size=(1008, 1008), mode='bilinear', align_corners=False)
        else:
            x_in = x

        # Ù‡Ù†Ø¯Ù„ Ú©Ø±Ø¯Ù† ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ù…Ø¯Ù„ (Ø¨Ø§ ÛŒØ§ Ø¨Ø¯ÙˆÙ† caption)
        try:
            features = self.image_encoder(x_in, captions=dummy_captions)
        except TypeError:
            features = self.image_encoder(x_in)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡
        if isinstance(features, dict):
            last_key = list(features.keys())[-1]
            return features[last_key]
        elif isinstance(features, (list, tuple)):
            return features[-1]
            
        return features

def get_text_features(clip_model, depth_classes, obj_classes, templates):
    """
    ØªÙˆÙ„ÛŒØ¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ CLIP Ø¨Ø±Ø§ÛŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù‚.
    """
    zeroshot_weights = []
    with torch.no_grad(): # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙÙ‚Ø· ÛŒÚ©Ø¨Ø§Ø± Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯
        for depth in depth_classes:
            for obj in obj_classes:
                texts = [template.format(obj, depth) for template in templates]
                texts = clip.tokenize(texts).to(DEVICE)
                
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² float32 Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§
                class_embeddings = clip_model.encode_text(texts).to(torch.float32) 
                
                class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)
                class_embedding = class_embeddings.mean(dim=0)
                class_embedding /= class_embedding.norm()
                zeroshot_weights.append(class_embedding)
    
    # Ø§Ø³ØªÚ© Ú©Ø±Ø¯Ù† Ùˆ Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† Ø§Ø² Ú¯Ø±Ø§Ù Ù…Ø­Ø§Ø³Ø¨Ø§ØªÛŒ (Detached)
    zeroshot_weights = torch.stack(zeroshot_weights, dim=1).to(DEVICE).to(torch.float32)
    return zeroshot_weights

class FCLayer(nn.Module):
    """
    Ù„Ø§ÛŒÙ‡ Ø¢Ø¯Ø§Ù¾ØªÙˆØ± Ø³Ø§Ø¯Ù‡ (MLP)
    """
    def __init__(self, c_in, reduction=4):
        super(FCLayer, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(c_in, c_in // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(c_in // reduction, c_in, bias=False),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.fc(x)

class MonoCLIP(nn.Module):
    def __init__(self):
        super(MonoCLIP, self).__init__()
        self.bins = len(depth_classes)

        print("Loading CLIP (RN50) for text encoding...")
        self.clip_model, _ = clip.load("RN50", device=LOAD_DEVICE)
        
        # ÙØ±ÛŒØ² Ú©Ø±Ø¯Ù† CLIP
        for param in self.clip_model.parameters():
            param.requires_grad = False
            
        if DEVICE == 'cuda':
            self.clip_model.to(DEVICE)

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ (ÙÛŒÚ©Ø³ Ø´Ø¯Ù‡)
        self.text_f = get_text_features(self.clip_model, depth_classes, obj_classes, depth_templates)
        self.text_dim = 1024

        # Ù„ÙˆØ¯ SAM Encoder
        self.sam_encoder = SAM3Encoder(SAM3_CHECKPOINT)
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¨Ø¹Ø§Ø¯ Ø®Ø±ÙˆØ¬ÛŒ SAM Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª Ø¢Ø¯Ø§Ù¾ØªÙˆØ±
        dummy = torch.randn(1, 3, 1008, 1008).to(DEVICE)
        with torch.no_grad():
            out = self.sam_encoder(dummy)
        
        self.visual_dim = out.shape[-1] # ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ú©Ø§Ù†Ø§Ù„ Ø¯Ø± Ø¯Ø§ÛŒÙ…Ù†Ø´Ù† Ø¢Ø®Ø± Ø§Ø³Øª
        print(f"âœ… SAM 3 Output Shape: {out.shape}")
        print(f"âœ… Visual Dimension: {self.visual_dim}")

        # --- ØªØ¹Ø±ÛŒÙ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´ ---
        self.adapter = FCLayer(self.visual_dim).to(DEVICE)
        
        if self.visual_dim != self.text_dim:
            self.vis_to_text = nn.Linear(self.visual_dim, self.text_dim, bias=False).to(DEVICE)
        else:
            self.vis_to_text = nn.Identity()

    def forward(self, x):
        # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø² SAM (ÙØ±ÛŒØ² Ø´Ø¯Ù‡)
        img_f = self.sam_encoder(x)
        
        # 2. ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ float32 Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒØ¯Ø§Ø±ÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ùˆ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² NaN
        img_f = img_f.to(torch.float32)

        # -----------------------------------------------------------
        # ğŸ”¥ FIX CRITICAL: Ø§Ø¹Ù…Ø§Ù„ Ø¢Ø¯Ø§Ù¾ØªÙˆØ± Ø¨Ù‡ ØµÙˆØ±Øª Residual
        # Ø§ÛŒÙ† Ø®Ø· Ø¨Ø§Ø¹Ø« Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ø¬Ø±ÛŒØ§Ù† Ù¾ÛŒØ¯Ø§ Ú©Ù†Ø¯ Ùˆ Ù„Ø§Ø³ Ú©Ù… Ø´ÙˆØ¯
        # -----------------------------------------------------------
        img_f = img_f + self.adapter(img_f)
        
        # 3. Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ùˆ ØªØºÛŒÛŒØ± Ø´Ú©Ù„ (Reshape)
        # Ø§Ú¯Ø± Ø®Ø±ÙˆØ¬ÛŒ 3 Ø¨Ø¹Ø¯ÛŒ Ø§Ø³Øª (B, Seq, Dim) ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª ØªØµÙˆÛŒØ± (B, H, W, Dim)
        if img_f.dim() == 3:
            # ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ spatial dimension ÙØ´Ø±Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³ØªØŒ Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø§Ø² Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            # Ù†Ú©ØªÙ‡: Ø§Ú¯Ø± Ù„Ø§Ø¬ÛŒÚ© Ø®Ø§ØµÛŒ Ø¨Ø±Ø§ÛŒ SAM3 Ø¯Ø§Ø±ÛŒØ¯ Ø§ÛŒÙ†Ø¬Ø§ Ú†Ú© Ú©Ù†ÛŒØ¯. 
            # Ø¯Ø± Ú©Ø¯ Ù‚Ø¨Ù„ÛŒ Ø´Ù…Ø§ Ø§ÛŒÙ†Ø·ÙˆØ± Ø¨ÙˆØ¯:
            img_f = img_f.transpose(1, 2)  # (B, Dim, Seq)
            img_f = img_f.unsqueeze(-1)    # (B, Dim, Seq, 1) - ØªØ¨Ø¯ÛŒÙ„ Ù…ÙˆÙ‚Øª Ø¨Ù‡ 4D
        
        img_f = img_f / (img_f.norm(dim=1, keepdim=True) + 1e-6)

        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª (B, H, W, C) Ø¨Ø±Ø§ÛŒ Ø¹Ø¨ÙˆØ± Ø§Ø² Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø®Ø·ÛŒ
        if img_f.shape[1] == self.visual_dim: # Ø§Ú¯Ø± Ú©Ø§Ù†Ø§Ù„ Ø¯Ø± Ø¯Ø§ÛŒÙ…Ù†Ø´Ù† 1 Ø§Ø³Øª
             img_f = img_f.permute(0, 2, 3, 1) 
        
        # 4. Ù¾Ø±ÙˆØ¬Ú©Ø´Ù† Ø¨Ù‡ ÙØ¶Ø§ÛŒ Ù…ØªÙ†
        img_f = self.vis_to_text(img_f)
        
        # 5. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ø¹Ù…Ù‚ (Ø´Ø¨Ø§Ù‡Øª ØªØµÙˆÛŒØ± Ùˆ Ù…ØªÙ†)
        depth_logits = 100. * img_f @ self.text_f
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Softmax (B, Classes, H, W)
        depth_logits = depth_logits.permute(0, 3, 1, 2)
        
        depth_logits /= temperature
        depth_probs = F.softmax(depth_logits, dim=1)
        
        # 6. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ù‚Ø´Ù‡ Ø¹Ù…Ù‚ Ù†Ù‡Ø§ÛŒÛŒ (Weighted Sum)
        bin_tensor = torch.tensor(bin_list).to(depth_probs.device)
        depth_map = (depth_probs * bin_tensor.view(1, -1, 1, 1)).sum(dim=1, keepdim=True)
        
        # 7. Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø¨Ù‡ Ø³Ø§ÛŒØ² Ø§ØµÙ„ÛŒ ØªØµÙˆÛŒØ± ÙˆØ±ÙˆØ¯ÛŒ
        if depth_map.shape[-2:] != x.shape[-2:]:
            depth_map = F.interpolate(depth_map, size=x.shape[-2:], mode='bilinear', align_corners=False)

        return depth_map