import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import sys
import clip

# --- ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ…Ù¾ÙˆØ±Øª SAM 3 ---
try:
    from sam3.model_builder import build_sam3_image_model
    # print("âœ… SAM 3 library imported successfully.")
except ImportError as e:
    print(f"âŒ Error importing SAM 3: {e}")
    sys.exit(1)

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¯Ø³ØªÚ¯Ø§Ù‡ ---
LOAD_DEVICE = 'cpu' # Ù„ÙˆØ¯ Ø§ÙˆÙ„ÛŒÙ‡ Ø±ÙˆÛŒ CPU Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² OOM
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø³ÛŒØ± Ùˆ Ù…Ø¯Ù„ ---
SAM3_CHECKPOINT = "/home/ram112/projects/def-jieliang/ram112/checkpoints/sam3_large.pth"

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª DepthCLIP
depth_templates = ['This {} is {}']
obj_classes = ['object']
depth_classes = ['giant', 'extremely close', 'close', 'not in distance', 'a little remote', 'far', 'unseen']
bin_list = [1.00, 1.50, 2.00, 2.25, 2.50, 2.75, 3.00]
temperature = 0.1

class SAM3Encoder(nn.Module):
    def __init__(self, checkpoint_path):
        super().__init__()
        print(f"Loading SAM 3 Image Model from: {checkpoint_path}")
        
        self.model = build_sam3_image_model(
            checkpoint_path=checkpoint_path,
            device=LOAD_DEVICE,  
            eval_mode=True,
            enable_segmentation=False,
            enable_inst_interactivity=False
        )
        
        if DEVICE == 'cuda':
            self.model.to(DEVICE)

        if hasattr(self.model.backbone, 'visual'):
            self.image_encoder = self.model.backbone.visual
        elif hasattr(self.model.backbone, 'trunk'):
            self.image_encoder = self.model.backbone.trunk
        else:
            self.image_encoder = self.model.backbone
        
        for param in self.model.parameters():
            param.requires_grad = False

    def forward(self, x):
        batch_size = x.shape[0]
        dummy_captions = [''] * batch_size
        
        if x.shape[-2:] != (1008, 1008):
            x_in = F.interpolate(x, size=(1008, 1008), mode='bilinear', align_corners=False)
        else:
            x_in = x

        try:
            features = self.image_encoder(x_in, captions=dummy_captions)
        except TypeError:
            features = self.image_encoder(x_in)
        
        if isinstance(features, dict):
            last_key = list(features.keys())[-1]
            return features[last_key]
        elif isinstance(features, (list, tuple)):
            return features[-1]
            
        return features

def get_text_features(clip_model, depth_classes, obj_classes, templates):
    zeroshot_weights = []
    with torch.no_grad():
        for depth in depth_classes:
            for obj in obj_classes:
                texts = [template.format(obj, depth) for template in templates]
                texts = clip.tokenize(texts).to(DEVICE)
                class_embeddings = clip_model.encode_text(texts).to(torch.float32) 
                class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)
                class_embedding = class_embeddings.mean(dim=0)
                class_embedding /= class_embedding.norm()
                zeroshot_weights.append(class_embedding)
    
    zeroshot_weights = torch.stack(zeroshot_weights, dim=1).to(DEVICE).to(torch.float32)
    return zeroshot_weights

class DepthAdapterCNN(nn.Module):
    """
    Ø¢Ø¯Ø§Ù¾ØªÙˆØ± Ø¬Ø¯ÛŒØ¯ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± CNN Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ú© ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ú©Ø§Ù†ÛŒ (Spatial Features).
    Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† FCLayer Ù‚Ø¯ÛŒÙ…ÛŒ Ø´Ø¯.
    """
    def __init__(self, c_in, reduction=4):
        super(DepthAdapterCNN, self).__init__()
        
        reduced_dim = max(c_in // reduction, 64) # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®ÛŒÙ„ÛŒ Ú©ÙˆÚ†Ú© Ø´Ø¯Ù† Ø§Ø¨Ø¹Ø§Ø¯
        
        self.adapter = nn.Sequential(
            # 1. Ú©Ø§Ù‡Ø´ Ø§Ø¨Ø¹Ø§Ø¯ (1x1 Conv)
            nn.Conv2d(c_in, reduced_dim, kernel_size=1, bias=False),
            nn.BatchNorm2d(reduced_dim),
            nn.ReLU(inplace=True),
            
            # 2. Ø¯Ø±Ú© Ù…Ú©Ø§Ù†ÛŒ (3x3 Conv) -> Ø§ÛŒÙ†Ø¬Ø§Ø³Øª Ú©Ù‡ Ø¹Ù…Ù‚ ÙÙ‡Ù…ÛŒØ¯Ù‡ Ù…ÛŒØ´Ù‡
            nn.Conv2d(reduced_dim, reduced_dim, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(reduced_dim),
            nn.ReLU(inplace=True),
            
            # 3. Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø¨Ø¹Ø§Ø¯ (1x1 Conv)
            nn.Conv2d(reduced_dim, c_in, kernel_size=1, bias=False),
            nn.BatchNorm2d(c_in)
            # Ù†Ú©ØªÙ‡: ReLU Ø¢Ø®Ø± Ø±Ùˆ Ø¨Ø±Ù…ÛŒØ¯Ø§Ø±ÛŒÙ… ØªØ§ Ø¨ØªÙˆÙ†Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ù†ÙÛŒ Ø±Ùˆ Ù‡Ù… Ø¯Ø± Residual Ø§ØµÙ„Ø§Ø­ Ú©Ù†Ù‡
        )
        
        # Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹ Ù†Ø±Ù… (Ù†Ø²Ø¯ÛŒÚ© Ø¨Ù‡ ØµÙØ±)
        # Ø§ÛŒÙ† Ø¨Ø§Ø¹Ø« Ù…ÛŒØ´Ù‡ Ø¯Ø± Ø´Ø±ÙˆØ¹ Ú©Ø§Ø±ØŒ ØªØ§Ø«ÛŒØ± Ø¢Ø¯Ø§Ù¾ØªÙˆØ± Ú©Ù… Ø¨Ø§Ø´Ù‡ Ùˆ Ù…Ø¯Ù„ Ù…Ù†ÙØ¬Ø± Ù†Ø´Ù‡
        nn.init.constant_(self.adapter[-1].weight, 0) 

    def forward(self, x):
        # ÙˆØ±ÙˆØ¯ÛŒ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨Ù‡ Ø´Ú©Ù„ (B, H, W, C) Ø§Ø³Øª
        # Ú©Ø§Ù†ÙˆÙ„ÙˆØ´Ù† Ù†ÛŒØ§Ø² Ø¨Ù‡ (B, C, H, W) Ø¯Ø§Ø±Ø¯
        
        x = x.permute(0, 3, 1, 2) # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª Ú©Ø§Ù†Ø§Ù„-Ø§ÙˆÙ„
        x = self.adapter(x)
        x = x.permute(0, 2, 3, 1) # Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø¨Ù‡ ÙØ±Ù…Øª Ú©Ø§Ù†Ø§Ù„-Ø¢Ø®Ø±
        
        return x

class MonoCLIP(nn.Module):
    def __init__(self):
        super(MonoCLIP, self).__init__()
        self.bins = len(depth_classes)

        print("Loading CLIP (RN50) for text encoding...")
        self.clip_model, _ = clip.load("RN50", device=LOAD_DEVICE)
        
        for param in self.clip_model.parameters():
            param.requires_grad = False
            
        if DEVICE == 'cuda':
            self.clip_model.to(DEVICE)

        self.text_f = get_text_features(self.clip_model, depth_classes, obj_classes, depth_templates)
        self.text_dim = 1024

        self.sam_encoder = SAM3Encoder(SAM3_CHECKPOINT)
        
        # Ú†Ú© Ú©Ø±Ø¯Ù† Ø³Ø§ÛŒØ² Ø®Ø±ÙˆØ¬ÛŒ
        dummy = torch.randn(1, 3, 1008, 1008).to(DEVICE)
        with torch.no_grad():
            out = self.sam_encoder(dummy)
        
        # Ø§Ú¯Ø± Ø®Ø±ÙˆØ¬ÛŒ 3 Ø¨Ø¹Ø¯ÛŒ Ø¨ÙˆØ¯ (Batch, Seq, Dim)ØŒ Ø¨Ø§ÛŒØ¯ Ø³Ø§ÛŒØ² ØªØµÙˆÛŒØ± Ø±Ùˆ Ø­Ø¯Ø³ Ø¨Ø²Ù†ÛŒÙ…
        if out.dim() == 3:
            self.visual_dim = out.shape[-1]
            # Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ SAM3 Ø®Ø±ÙˆØ¬ÛŒ 64x64 Ù…ÛŒØ¯Ù‡ Ø§Ú¯Ø± ÙˆØ±ÙˆØ¯ÛŒ 1024 Ø¨Ø§Ø´Ù‡ (Patch Size 16)
            self.spatial_size = int(out.shape[1] ** 0.5) 
        else:
            self.visual_dim = out.shape[1] # Ø§Ú¯Ø± (B, C, H, W) Ø¨Ø§Ø´Ù‡
            
        print(f"âœ… SAM 3 Output Shape: {out.shape}")
        print(f"âœ… Visual Dimension: {self.visual_dim}")

        # --- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ø¯Ø§Ù¾ØªÙˆØ± CNN Ø¬Ø¯ÛŒØ¯ ---
        self.adapter = DepthAdapterCNN(self.visual_dim).to(DEVICE)
        
        if self.visual_dim != self.text_dim:
            self.vis_to_text = nn.Linear(self.visual_dim, self.text_dim, bias=False).to(DEVICE)
        else:
            self.vis_to_text = nn.Identity()

    def forward(self, x):
        # 1. ÙÛŒÚ†Ø±Ù‡Ø§ Ø§Ø² SAM
        img_f = self.sam_encoder(x)
        img_f = img_f.to(torch.float32)

        # 2. Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø³Ø§Ø²ÛŒ Ø´Ú©Ù„ ØªÙ†Ø³ÙˆØ± Ø¨Ù‡ (B, H, W, C)
        if img_f.dim() == 3: # Ø§Ú¯Ø± (B, Seq, C) Ø¨ÙˆØ¯
            B, Seq, C = img_f.shape
            H = W = int(Seq ** 0.5)
            img_f = img_f.view(B, H, W, C)
        elif img_f.dim() == 4 and img_f.shape[1] == self.visual_dim: # Ø§Ú¯Ø± (B, C, H, W) Ø¨ÙˆØ¯
            img_f = img_f.permute(0, 2, 3, 1)

        # 3. Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
        img_f = img_f / (img_f.norm(dim=-1, keepdim=True) + 1e-6)

        # -----------------------------------------------------------
        # ğŸ”¥ CNN ADAPTER (Residual)
        # -----------------------------------------------------------
        img_f = img_f + self.adapter(img_f)
        
        # 4. Ù¾Ø±ÙˆØ¬Ú©Ø´Ù† Ø¨Ù‡ ÙØ¶Ø§ÛŒ Ù…ØªÙ†
        img_f = self.vis_to_text(img_f)
        
        # 5. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¹Ù…Ù‚
        depth_logits = 100. * img_f @ self.text_f
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ (B, Classes, H, W) Ø¨Ø±Ø§ÛŒ Softmax
        depth_logits = depth_logits.permute(0, 3, 1, 2)
        
        depth_logits /= temperature
        depth_probs = F.softmax(depth_logits, dim=1)
        
        bin_tensor = torch.tensor(bin_list).to(depth_probs.device)
        depth_map = (depth_probs * bin_tensor.view(1, -1, 1, 1)).sum(dim=1, keepdim=True)
        
        if depth_map.shape[-2:] != x.shape[-2:]:
            depth_map = F.interpolate(depth_map, size=x.shape[-2:], mode='bilinear', align_corners=False)

        return depth_map